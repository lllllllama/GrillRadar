#!/usr/bin/env python3
"""
ç›´æ¥æµ‹è¯•GitHubçˆ¬è™«çš„trendingåŠŸèƒ½
"""
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

import httpx
from bs4 import BeautifulSoup
import re

def parse_github_number(text: str) -> int:
    """è§£æGitHubçš„æ•°å­—æ ¼å¼"""
    text = text.strip().replace(',', '').replace(' ', '')
    if 'k' in text.lower():
        return int(float(text.lower().replace('k', '')) * 1000)
    try:
        return int(text)
    except:
        return 0

def main():
    print("=" * 70)
    print("GitHub Trending Crawler - Direct Test")
    print("=" * 70)
    print()

    # æµ‹è¯•trendingé¡µé¢ï¼ˆä¸å¸¦è¯­è¨€è¿‡æ»¤ï¼‰
    url = "https://github.com/trending?since=daily"
    print(f"ğŸ“¡ Fetching: {url}")

    response = httpx.get(url, timeout=10, follow_redirects=True)
    print(f"âœ“ Status: {response.status_code}")
    print()

    soup = BeautifulSoup(response.text, 'html.parser')
    repo_articles = soup.find_all('article', class_='Box-row')
    print(f"âœ“ Found {len(repo_articles)} articles")
    print()

    items_parsed = 0
    for i, article in enumerate(repo_articles[:5]):
        try:
            print(f"ğŸ“¦ Repository {i+1}:")
            print("-" * 70)

            # æå–repoåç§°å’ŒURL
            h2 = article.find('h2', class_='h3')
            if not h2:
                print("   âŒ No h2 found")
                continue

            link = h2.find('a')
            if not link:
                print("   âŒ No link found")
                continue

            repo_path = link.get('href', '').strip()
            repo_url = f"https://github.com{repo_path}"
            repo_name = link.get_text(strip=True)

            print(f"   Name: {repo_name}")
            print(f"   URL: {repo_url}")

            # æå–æè¿°
            desc_p = article.find('p', class_='col-9')
            description = desc_p.get_text(strip=True) if desc_p else ""
            print(f"   Description: {description[:100]}...")

            # æå–staræ•°
            star_link = article.find('a', href=lambda x: x and '/stargazers' in x)
            if star_link:
                star_text = star_link.get_text(strip=True)
                star_count = parse_github_number(star_text)
                print(f"   Stars: {star_text} â†’ {star_count}")
            else:
                print(f"   Stars: NOT FOUND")

            # æå–è¯­è¨€
            lang_color = article.find('span', class_='repo-language-color')
            if lang_color:
                # å°è¯•å¤šç§æ–¹å¼è·å–è¯­è¨€å
                lang_text = lang_color.next_sibling
                language_name = "Unknown"
                if isinstance(lang_text, str):
                    language_name = lang_text.strip()
                elif hasattr(lang_text, 'get_text'):
                    language_name = lang_text.get_text(strip=True)

                # å¦‚æœnext_siblingä¸workï¼Œå°è¯•æ‰¾çˆ¶å…ƒç´ ä¸­çš„æ–‡æœ¬
                if language_name == "Unknown" or not language_name:
                    parent = lang_color.parent
                    if parent:
                        # ç§»é™¤é¢œè‰²spanï¼Œå‰©ä¸‹çš„å°±æ˜¯è¯­è¨€å
                        parent_copy = parent.__copy__()
                        for span in parent_copy.find_all('span', class_='repo-language-color'):
                            span.decompose()
                        language_name = parent_copy.get_text(strip=True)

                print(f"   Language: {language_name}")
            else:
                print(f"   Language: NOT FOUND (no color span)")

            print(f"   âœ“ Parsed successfully")
            items_parsed += 1

        except Exception as e:
            print(f"   âŒ Error: {e}")
            import traceback
            traceback.print_exc()

        print()

    print("=" * 70)
    print(f"âœ¨ Parsing complete: {items_parsed}/{min(5, len(repo_articles))} items parsed")

if __name__ == '__main__':
    main()
