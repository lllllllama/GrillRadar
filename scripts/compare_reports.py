#!/usr/bin/env python3
"""
Report Comparison Script

Compare two reports (e.g., old vs new engine) to detect improvements or regressions.

Usage:
    # Compare two saved reports
    python scripts/compare_reports.py report_v1.json report_v2.json

    # Compare with custom names
    python scripts/compare_reports.py \\
      --baseline report_v1.json \\
      --candidate report_v2.json \\
      --names "v1.0" "v2.0-refactored"

    # Export comparison to JSON
    python scripts/compare_reports.py report_v1.json report_v2.json --output comparison.json

    # Export comparison to markdown
    python scripts/compare_reports.py report_v1.json report_v2.json --markdown comparison.md
"""
import argparse
import json
import sys
from pathlib import Path
from typing import Optional

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from app.models.report import Report
from app.eval.report_quality import compare_reports, format_comparison


def load_report(report_path: Path) -> Report:
    """Load report from JSON file"""
    with open(report_path, 'r', encoding='utf-8') as f:
        report_data = json.load(f)

    # Handle both Pydantic v1 and v2
    if hasattr(Report, 'model_validate'):
        # Pydantic v2
        return Report.model_validate(report_data)
    else:
        # Pydantic v1
        return Report.parse_obj(report_data)


def save_comparison_json(comparison_dict: dict, output_path: Path):
    """Save comparison result to JSON file"""
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(comparison_dict, f, ensure_ascii=False, indent=2)


def save_comparison_markdown(comparison_text: str, output_path: Path):
    """Save comparison result to Markdown file"""
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(comparison_text)


def generate_markdown_report(
    comparison_text: str,
    baseline_name: str,
    candidate_name: str,
    baseline_path: Path,
    candidate_path: Path
) -> str:
    """
    Generate comprehensive markdown report with comparison results

    Args:
        comparison_text: Formatted comparison text
        baseline_name: Name of baseline report
        candidate_name: Name of candidate report
        baseline_path: Path to baseline report file
        candidate_path: Path to candidate report file

    Returns:
        Markdown formatted report
    """
    lines = []
    lines.append("# Report Comparison")
    lines.append("")
    lines.append("## Metadata")
    lines.append("")
    lines.append(f"- **Baseline**: `{baseline_name}` ({baseline_path})")
    lines.append(f"- **Candidate**: `{candidate_name}` ({candidate_path})")
    lines.append("")
    lines.append("## Comparison Results")
    lines.append("")
    lines.append("```")
    lines.append(comparison_text)
    lines.append("```")
    lines.append("")
    lines.append("---")
    lines.append("")
    lines.append("*Generated by GrillRadar Evaluation Toolkit*")
    lines.append("")

    return "\n".join(lines)


def run_comparison(
    baseline_path: Path,
    candidate_path: Path,
    baseline_name: Optional[str] = None,
    candidate_name: Optional[str] = None,
    output_json: Optional[Path] = None,
    output_markdown: Optional[Path] = None
):
    """
    Compare two reports and display results

    Args:
        baseline_path: Path to baseline report JSON
        candidate_path: Path to candidate report JSON
        baseline_name: Display name for baseline
        candidate_name: Display name for candidate
        output_json: Optional path to save comparison as JSON
        output_markdown: Optional path to save comparison as Markdown
    """
    # Default names from file names
    if not baseline_name:
        baseline_name = baseline_path.stem
    if not candidate_name:
        candidate_name = candidate_path.stem

    print(f"üìÇ Loading baseline: {baseline_path}")
    baseline_report = load_report(baseline_path)

    print(f"üìÇ Loading candidate: {candidate_path}")
    candidate_report = load_report(candidate_path)

    print(f"üîç Comparing reports...")
    print()

    # Run comparison
    comparison = compare_reports(baseline_report, candidate_report)

    # Format and display
    formatted = format_comparison(comparison, baseline_name, candidate_name)
    print(formatted)

    # Save outputs if requested
    if output_json:
        print(f"üíæ Saving comparison to JSON: {output_json}")
        save_comparison_json(comparison.to_dict(), output_json)

    if output_markdown:
        print(f"üíæ Saving comparison to Markdown: {output_markdown}")
        markdown_content = generate_markdown_report(
            comparison_text=formatted,
            baseline_name=baseline_name,
            candidate_name=candidate_name,
            baseline_path=baseline_path,
            candidate_path=candidate_path
        )
        save_comparison_markdown(markdown_content, output_markdown)


def main():
    parser = argparse.ArgumentParser(
        description="Compare two reports to detect improvements or regressions",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Compare two saved reports
  python scripts/compare_reports.py report_v1.json report_v2.json

  # Compare with custom names
  python scripts/compare_reports.py \\
    --baseline report_v1.json \\
    --candidate report_v2.json \\
    --names "v1.0" "v2.0-refactored"

  # Export comparison to JSON
  python scripts/compare_reports.py report_v1.json report_v2.json --output comparison.json

  # Export comparison to markdown
  python scripts/compare_reports.py report_v1.json report_v2.json --markdown comparison.md
        """
    )

    # Report paths (positional or named)
    parser.add_argument(
        'baseline',
        nargs='?',
        type=Path,
        help='Path to baseline report JSON'
    )
    parser.add_argument(
        'candidate',
        nargs='?',
        type=Path,
        help='Path to candidate report JSON'
    )

    # Named arguments (alternative)
    parser.add_argument(
        '--baseline',
        dest='baseline_named',
        type=Path,
        help='Path to baseline report JSON (alternative to positional)'
    )
    parser.add_argument(
        '--candidate',
        dest='candidate_named',
        type=Path,
        help='Path to candidate report JSON (alternative to positional)'
    )

    # Display names
    parser.add_argument(
        '--names',
        nargs=2,
        metavar=('BASELINE_NAME', 'CANDIDATE_NAME'),
        help='Display names for reports (e.g., "v1.0" "v2.0")'
    )

    # Output options
    parser.add_argument(
        '--output',
        type=Path,
        help='Path to save comparison as JSON'
    )
    parser.add_argument(
        '--markdown',
        type=Path,
        help='Path to save comparison as Markdown'
    )

    args = parser.parse_args()

    # Resolve baseline and candidate paths
    baseline_path = args.baseline or args.baseline_named
    candidate_path = args.candidate or args.candidate_named

    if not baseline_path or not candidate_path:
        parser.error("Both baseline and candidate report paths are required")

    # Validate paths
    if not baseline_path.exists():
        print(f"‚ùå Baseline report not found: {baseline_path}", file=sys.stderr)
        sys.exit(1)
    if not candidate_path.exists():
        print(f"‚ùå Candidate report not found: {candidate_path}", file=sys.stderr)
        sys.exit(1)

    # Parse custom names
    baseline_name = args.names[0] if args.names else None
    candidate_name = args.names[1] if args.names else None

    # Run comparison
    try:
        run_comparison(
            baseline_path=baseline_path,
            candidate_path=candidate_path,
            baseline_name=baseline_name,
            candidate_name=candidate_name,
            output_json=args.output,
            output_markdown=args.markdown
        )
        sys.exit(0)
    except Exception as e:
        print(f"‚ùå Comparison failed: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()
