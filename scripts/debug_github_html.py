#!/usr/bin/env python3
"""
Debug GitHub Trending Page HTML Structure
Ëé∑ÂèñÂπ∂ÂàÜÊûêGitHub trendingÈ°µÈù¢ÁöÑÂÆûÈôÖHTMLÁªìÊûÑ
"""
import httpx
from bs4 import BeautifulSoup

def main():
    print("=" * 70)
    print("GitHub Trending Page HTML Structure Analysis")
    print("=" * 70)
    print()

    # Ëé∑ÂèñGitHub trendingÈ°µÈù¢
    url = "https://github.com/trending?since=daily"
    print(f"üì° Fetching: {url}")

    try:
        response = httpx.get(url, timeout=10, follow_redirects=True)
        print(f"‚úì Status: {response.status_code}")
        print(f"‚úì Content length: {len(response.text)} chars")
        print()

        # Ëß£ÊûêHTML
        soup = BeautifulSoup(response.text, 'html.parser')

        # Êü•ÊâæÂèØËÉΩÁöÑ‰ªìÂ∫ìÂÆπÂô®
        print("üîç Looking for repository containers...")
        print()

        # Â∞ùËØïÂ§öÁßçÂèØËÉΩÁöÑÈÄâÊã©Âô®
        selectors = [
            ('article.Box-row', 'Original selector (article.Box-row)'),
            ('article', 'All articles'),
            ('div.Box-row', 'div.Box-row'),
            ('h2.h3', 'h2.h3 (repo name headers)'),
            ('h2', 'All h2 elements'),
            ('.Box-row', 'Class: Box-row'),
            ('[class*="Box"]', 'Any element with Box in class'),
            ('[class*="repo"]', 'Any element with repo in class'),
        ]

        for selector, description in selectors:
            elements = soup.select(selector)
            print(f"  {selector:30s} [{description:40s}]: {len(elements):3d} found")

        print()
        print("=" * 70)

        # ‰øùÂ≠òHTMLÂà∞Êñá‰ª∂Áî®‰∫éËØ¶ÁªÜÂàÜÊûê
        output_file = "/tmp/github_trending.html"
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(response.text)
        print(f"üíæ Full HTML saved to: {output_file}")
        print()

        # Êü•ÊâæÂâçÂá†‰∏™ÁúãËµ∑Êù•ÂÉè‰ªìÂ∫ìÁöÑÂÖÉÁ¥†
        print("üìù Sample repository-like elements:")
        print()

        # Â∞ùËØïÊâæÂà∞ÂåÖÂê´repoÈìæÊé•ÁöÑÂÖÉÁ¥†
        repo_links = soup.find_all('a', href=lambda x: x and '/' in str(x) and not x.startswith('http'))
        print(f"  Found {len(repo_links)} internal links")

        # ÊòæÁ§∫Ââç5‰∏™ÁúãËµ∑Êù•ÂÉè‰ªìÂ∫ìÁöÑÈìæÊé•
        repo_count = 0
        for link in repo_links[:50]:
            href = link.get('href', '')
            # GitHub‰ªìÂ∫ìÈìæÊé•Ê†ºÂºè: /owner/repo
            if href.count('/') == 2 and not href.startswith('/topics') and not href.startswith('/trending'):
                repo_count += 1
                text = link.get_text(strip=True)
                print(f"  {repo_count}. {href:50s} | {text[:40]}")
                if repo_count >= 10:
                    break

        print()
        print("=" * 70)
        print("‚ú® Analysis complete!")

    except Exception as e:
        print(f"‚ùå Error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    main()
